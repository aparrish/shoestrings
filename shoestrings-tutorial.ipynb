{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b7301c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf802f",
   "metadata": {},
   "source": [
    "# Shoestrings tutorial and cookbook\n",
    "\n",
    "By [Allison Parrish](https://www.decontextualize.com/)\n",
    "\n",
    "Shoestrings is a [Markov chain text generation](https://en.wikipedia.org/wiki/Markov_chain#Markov_text_generators) library for Python, with a particular focus on creative text generation. The library implements a simple ngram-based language model and uses the model to chain together predictions about what word will come next. Shoestrings is designed to be performant (using sparse matrices and numpy arrays internally) while remaining easy-to-use for beginners. Inspired by some common features of large language model generation libraries, Shoestrings includes implementations of some powerful features that are missing from other Markov chain text generation libraries out there, such as:\n",
    "\n",
    "* [Beam search](https://en.wikipedia.org/wiki/Beam_search) and [softmax sampling](https://en.wikipedia.org/wiki/Softmax_function) with temperature\n",
    "* Custom scoring criteria and stopping criteria for sequences (alongside the criterion of \"most likely\" according to the language model)\n",
    "* Easy implementation of language- or application-specific tokenization\n",
    "* Easy adaptation to tasks other than text generation\n",
    "\n",
    "This notebook goes over some of the highlights of the library, starting with the `TextGenerator` class, which wraps some of the more advanced features of the library for beginners and other folks who just want to get some text generation going.\n",
    "\n",
    "For a more general introduction to Markov chain text generation, see [this tutorial notebook I made for a different Markov chain generation library](https://github.com/aparrish/predictive-text-and-text-generation/blob/master/predictive-text-and-text-generation.ipynb). (Eventually I will incorporate much of this material into this notebook. All in due time, friends.)\n",
    "\n",
    "## Warning: this is early-release software!\n",
    "\n",
    "I'm putting this code on GitHub so I can use it in workshops, but it's not really ready for primetime yet. Please be aware that this is **barely alpha** software. Some stuff might be broken, and everything is subject to redesign and backwards incompatible changes at a moment's notice.\n",
    "\n",
    "## Why Markov chain generation?\n",
    "\n",
    "Let's say that the goal of making text with a computer is to create interesting new juxtapositions of bits of language that have never been produced before. Some goals alongside and complementary to this are to know where those juxtapositions came from, and to produce text that is meaningfully in dialogue with the sources and algorithms that brought that text into being. Markov chain text generation fits very well into an artistic process that has these goals—much better (in my opinion) than large language models. (You can [read more about my opinion here](https://posts.decontextualize.com/language-models-ransom-notes/).)\n",
    "\n",
    "Additionally, Markov chain text generation works well on tiny corpora (as small as a single word) and can be built quickly on very modest hardware—quickly enough that the time it takes to \"train\" a Markov model from scratch on a reasonably-sized corpus (say, a novel-length text) is all but instantaneous. This variety of text generation fits better in a tight ideate-experiment-revise feedback loop that suits artistic creation.\n",
    "\n",
    "## Working with a corpus\n",
    "\n",
    "In the examples below, I'm going to use the text of Mary Shelley's *Frankenstein*. [The full text version that I'm using is available online.](https://github.com/aparrish/plaintext-example-files/blob/master/frankenstein.txt) You're welcome to use your own plain-text corpus instead!\n",
    "\n",
    "## First steps\n",
    "\n",
    "You'll need to install the Shoestrings library, which you can do with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install https://github.com/aparrish/shoestrings/archive/main.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419ac88",
   "metadata": {},
   "source": [
    "Shoestrings has two major library dependencies: numpy and scipy. If you don't already have these installed, they should get installed as part of the installation process above.\n",
    "\n",
    "For this notebook, I also suggest installing the [sentencex](https://pypi.org/project/sentencex/) library for simple and fast sentence tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1159f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentencex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a499c24",
   "metadata": {},
   "source": [
    "### Splitting the text into sentences\n",
    "\n",
    "The following cell uses the `sentencex` library to open our source text and split it into a list of sentences. If you're using a different source file, put the file name in the call to `open()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a37c1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencex\n",
    "text = open(\"./frankenstein.txt\").read()\n",
    "sentences = sentencex.segment(\"en\", text) # sentencex supports languages other than english! see the docs\n",
    "# filter empty strings and fix mid-sentence line breaks\n",
    "sentences = [item.replace(\"\\n\", \" \") for item in sentences if len(item.strip()) > 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16cb55",
   "metadata": {},
   "source": [
    "I like to check the results of reading in a file by perusing the resulting data structure at random. Let's get a few random sentences from the source text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "88067352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Margaret, if you had seen the man who thus capitulated for his safety, your surprise would have been boundless.',\n",
       " 'In a few minutes after, I heard the creaking of my door, as if some one endeavoured to open it softly.',\n",
       " 'Despair had indeed almost secured her prey, and I should soon have sunk beneath this misery.',\n",
       " 'The astonishment which I had at first experienced on this discovery soon gave place to delight and rapture.',\n",
       " 'He came.',\n",
       " 'This, to my mother, was more than a duty; it was a necessity, a passion--remembering what she had suffered, and how she had been relieved--for her to act in her turn the guardian angel to the afflicted.',\n",
       " '\"Then I fancy we have seen him, for the day before we picked you up we saw some dogs drawing a sledge, with a man in it, across the ice.\"',\n",
       " 'She was thinner and had lost much of that heavenly vivacity that had before charmed me; but her gentleness and soft looks of compassion made her a more fit companion for one blasted and miserable as I was.',\n",
       " 'I remained silent.',\n",
       " 'I, their eldest child, was born at Naples, and as an infant accompanied them in their rambles.']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(sentences, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84737dcb",
   "metadata": {},
   "source": [
    "### Generate some text\n",
    "\n",
    "Now, we'll import the parts of the Shoestrings library we need to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "7b2fbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shoestrings import TextGenerator\n",
    "from shoestrings.tokenizers import SimpleWordTokenizer, SimpleCharacterTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d440e92",
   "metadata": {},
   "source": [
    "The `TextGenerator` class is instantiated with three things: the length of the ngram that we want to use, a tokenizer, and a list of strings to use as the source text. We'll talk more about the specifics of tokenizers below; for now, I'm just going to pass an instance of the `SimpleWordTokenizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "007e94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = TextGenerator(n=3, tokenizer=SimpleWordTokenizer(), source=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faca7e6",
   "metadata": {},
   "source": [
    "After creating the object, call its `.generate_one()` method to generate new stretches of text from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "62122304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miserable himself that he, the snow that obstructed me and said some words in a country of eternal light?'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.generate_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c9651",
   "metadata": {},
   "source": [
    "To generate a few lines, call this in a `for` loop and print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "702995ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clerval spoke thus as we hurried through the cottage and prepared the food, and I was destined to endure their present hardships.\n",
      "\n",
      "Strange and harrowing must be of use in undeceiving them.\n",
      "\n",
      "TO Mrs. Saville, England\n",
      "\n",
      "Could they turn from their sockets in attending him; he breathed with difficulty the words of Agatha were thrown, by insensible steps, not high, but she persisted, and do not feel for them on occasion to pass the night or hasten its conclusion by an eagerness which perpetually increased, I passed an hour in this peopled earth.\n",
      "\n",
      "What freedom?\n",
      "\n",
      "Beyond Cologne we descended to the existence of their miserable fare.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(gen.generate_one())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505a2e5",
   "metadata": {},
   "source": [
    "When generating text from the underlying language model, the `TextGenerator` object does its best to ensure that every text begins with an ngram that occurs at the beginning of a sentence somewhere in the corpus, and to ensure that every sentence ends at a sentence boundary (i.e., sentence-final punctuation like `.`, `!`, `?` and so forth.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6558687",
   "metadata": {},
   "source": [
    "For a character-based Markov chain rather than a word-based Markov chain, you can use `SimpleCharacterTokenizer` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "eca81a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_gen = TextGenerator(n=8, tokenizer=SimpleCharacterTokenizer(), source=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "81011217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All was called forth from the woods.\n"
     ]
    }
   ],
   "source": [
    "print(char_gen.generate_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aef573",
   "metadata": {},
   "source": [
    "But we'll stick with the word-based generator for the rest of these examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ef94e",
   "metadata": {},
   "source": [
    "### Temperature: the basics\n",
    "\n",
    "There are a few parameters that you can play with right off the bat. I'll go into some detail explaining how these parameters work later, but for now you can experiment with them to see what differences they make to the output. The first parameter is `temperature`, which allows you to adjust the shape of the probabilities at each step of the prediction process. I'll explain this in more detail below, but the gist is that as the temperature gets lower (approaching zero), the model will tend to pick tokens with already high probability; as the temperature gets *higher*, the model distinguishes less and less between the tokens' probability in the underlying data. A few examples will demonstrate. First, here are a few low-temperature generations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "01783a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poor Justine was the same time she gently deplored her own mind that the fiend should openly attack me. I am not so changeable as the sun had risen, he had been the favourite of her lover, instantly informed me of my father' s house.\n",
      "\n",
      "Dear mountains! my own heart.\n",
      "\n",
      "Yesterday the stranger.\n",
      "\n",
      "Under the guidance of my own heart, I was unable to solve them.\n",
      "\n",
      "As I spoke, rage sparkled in my own heart, but I was not, as I was unable to solve these questions, but I was unable to solve them.\n",
      "\n",
      "Harmony was the same time, but I was unable to solve these questions, but I was unable to solve these questions, but I was not so selfish.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(gen.generate_one(temperature=0.05))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12939ede",
   "metadata": {},
   "source": [
    "As you can see, at low temperatures (say, less than 0.5), the model tends to select common words and produce repetitive patterns, sometimes getting caught in \"loops\" for protracted periods. In some cases, it never predicts that the end of the sentence will arrive, and you see the model giving up and ceasing to adding tokens after the `max_tokens` limit (discussed below). At a high temperature, we find the opposite to be the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "23b797b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All, save me, until from the ocean he was, you will never consent.\"\n",
      "\n",
      "Write, dearest Victor, when wearied by a merchantman now on its homeward voyage from Archangel; more fortunate than I became more attentive and interested; I began the creation; I will confide this tale and his friendship was of little happiness who did not participate in these words were legible in one occupation that I dated my creation.\n",
      "\n",
      "Ay, stare if you obey me in my labour for the dreams of bliss that cannot be realized. What was I?\n",
      "\n",
      "St. Bernard' s attention, and almost endless journey across the fields with a panegyric upon modern chemistry, the oak had disappeared from the grave- worms crawling in the impotence of despair. But this was to procure and promising me the list of several days.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(gen.generate_one(temperature=5.0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22730603",
   "metadata": {},
   "source": [
    "Here the model seems to select more unusual turns of phrase and rarely gets caught in loops—instead, it tends to ramble on a bit. The default value for temperature is `1.0`, which causes the model to pick continuations at exactly the probability in which they occur in the source text. In practice, it's worthwhile to experiment with values around 1.0 (say, 0.35 up to 1.5, depending on your source text and ngram length) to find the sweet spot between repetition and unpredictability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd1bd9",
   "metadata": {},
   "source": [
    "### Beam search: the basics\n",
    "\n",
    "The second parameter worth talking about is `beam_width`, which is a parameter to the Shoestrings model's [beam search](https://en.wikipedia.org/wiki/Beam_search) algorithm. When the `beam_width` parameter is greater than 1, the `.generate_one()` function (and the other generation functions that we'll discuss below) internally keeps track of *several generated sequences at once*, instead of generating a single sequence. At each timestep, each of these sequences is evaluated for its combined probability, and the model keeps only the top `n` sequences (where `n` is equal to the `beam_width` parameter).\n",
    "\n",
    "You can think of this a little bit like a \"divide and conquer\" strategy. If you lost your keys your keys and were trying to find them, one strategy would be to go to the room where you believe you are most likely to have left them, and then look in the piece of furniture in that room where you were most likely to have left them, and then open the drawer in the piece of furniture where you were most likely to have left them, etc. A potentially better strategy might be to ask a few friends over, and assign each friend to look in the first, second, third (etc) most likely rooms where your keys might be, and then they in turn would look in the first, second, third (etc) most likely pieces of furniture in each room, and so forth. That's sort of like how a beam search works: we're trying to find the most likely sequence of words in the apartment of the model's possibility space.\n",
    "\n",
    "> *Pedants note*: the metaphor above describes a depth-first search strategy, while beam search is breadth-first. It's difficult to come up with real-life analogies for breadth-first search, sorry.\n",
    "\n",
    "In essence, a beam search is a way of attempting to maximize the probability of *the entire sequence*, rather than simply maximizing the probability of each ngram within the sequence. The default value for `beam_width` is 1, meaning that no beam search is performed at all. In practice, you can sometimes get more \"realistic\" sounding results by increasing the `beam_width` parameter, at the cost of time and memory use. Let's give it a shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "fabb8a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabeth saw even this last overwhelming event?\n",
      "\n",
      "During this interval, one by one glimmering and seemingly ineffectual light. The modern masters promise very little and conversed in broken accents:\" Unhappy man!\n",
      "\n",
      "His countenance instantly assumed an aspect expressive of disgust and affright.\n",
      "\n",
      "London was our darling and our pride!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(gen.generate_one(beam_width=25))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c45b1c",
   "metadata": {},
   "source": [
    "Hmm, well, okay, those don't really feel any different from normal generated sentences. Trust me, though, beam search is useful, and we'll see some better examples later on when we talk about custom scoring. I do find that beam search helps to \"tame\" some of the more outrageous word choices of high-temperature generation, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "bff48ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus Elizabeth endeavoured to soothe me when others would have introduced some other topic than that which preceded it.\n",
      "\n",
      "Life and death.\n",
      "\n",
      "Vegetables and bread, cheese, milk, and a linen jacket being her only garb; her words pierce my heart.\n",
      "\n",
      "Answer me, and taking his guitar, played some airs so entrancingly beautiful that they might not debar him from the shore.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(gen.generate_one(beam_width=25, temperature=3.0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf1de1",
   "metadata": {},
   "source": [
    "If you call the `.generate()` method instead of `.generate_one()`, you'll get *all* of the sequences that resulted from the beam search, rather than just the most likely one. The results are sorted in order from most likely to least likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5c49ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clouds hid the summits of its rage. Let your compassion be moved, and I ardently desired to divine.', 'Clouds hid the summits of its rage. Let your compassion be moved, and I ardently desired to divine.', 'Clouds hid the summits of its rage. Let your compassion be moved, and I ardently desired to divine.', 'Clouds hid the summits of its rage. Let your compassion be moved, and I ardently desired to divine.', 'Clouds hid the summits of its rage. Let your compassion be moved, and I ardently desired to divine.', 'Clouds hid the summits of its rage. Let your compassion be moved, and I ardently desired to divine.', 'Clouds hid the summits of its rage. Let your compassion be moved, and I ardently desired to plead, she replied that I was not splintered by the multitude of feelings which bore me onwards, like a mighty tide, overwhelmed every other circumstance of existence pass before me. I leave you, and I saw the mighty Alps, whose hands seem only made to a conclusion.', 'Clouds hid the summits of its rage. Let your compassion be moved, and I ardently desired to plead, she replied that I was not splintered by the multitude of feelings which bore me onwards, like a mighty tide, overwhelmed every other circumstance of existence pass before me. I leave you, and I saw the mighty Alps, whose hands seem only made to a conclusion.']\n"
     ]
    }
   ],
   "source": [
    "print(gen.generate(beam_width=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e60b6",
   "metadata": {},
   "source": [
    "### Constraining the number of tokens\n",
    "\n",
    "The `max_tokens` parameter tells the model to stop generating after the generated sequence has reached a particular number of tokens. This can be helpful when you want to ensure that you don't end up generating extremely long sequences, if the model happens to get caught in a loop. It can also be helpful for prototyping, if you don't want to spend a lot of time generating longer sequences just to check to see if an idea works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "90cadd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another storm enlightened Jura with faint flashes; and I was unable to solve them.\n",
      "Heaven bless my beloved country; but I was not so wretched a condition.\n",
      "Soft tears again bedewed my cheeks, and I was unable to solve them.\n",
      "No; but I was unable to solve these questions, but I was not the master said,\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(gen.generate_one(temperature=0.1, max_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d09b1a",
   "metadata": {},
   "source": [
    "(Note that this does *not* guarantee that the sequences will end with appropriate punctuation or an end-of-sentence token.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510d820",
   "metadata": {},
   "source": [
    "### Where generation begins\n",
    "\n",
    "By default, the `TextGenerator` object's `.generate()` and `.generate_one()` methods start with a randomly chosen ngram that was found at the beginning of any sentence in the training data. The `start_string` parameter lets you pick a string to start with instead. This can be helpful when you want a bunch of different possible generations that begin with the same sequence, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "be8d26dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was there to subdue me to speak, but it would be more than he pities me?\n",
      "\n",
      "She was not then cold.\n",
      "\n",
      "She was dressed in mourning, and now he instantly changed the subject of their victim and spared no pains or exertions on my misfortunes, I saw a wildness in my tastes for natural science.\n",
      "\n",
      "She was senseless, and nature again assume the barren and bleak appearance it had been committed at the bottom of the kitchen stove.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(gen.generate_one(start_string=\"She was\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95d9a5",
   "metadata": {},
   "source": [
    "It's important to note that the `start_string` argument *must occur somewhere in the source text*, and must be the same length in tokens as the model's ngram length minus one (e.g., for a word-tokenized model with ngram length 3, the `start_string` parameter must be two words). It would be fun if you could prompt a Markov chain text generator ChatGPT-style, but alas, it is not to be. If the tokens in the string do *not* occur in the source text, you'll get a key error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "ef47652a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hey'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-279-cc94793b35a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Hey Frankie\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/projects/shmarkov-new/shoestrings/__init__.py\u001b[0m in \u001b[0;36mgenerate_one\u001b[0;34m(self, temperature, beam_width, start_string, max_tokens, custom_scorer, processors, stoppers)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \"\"\"\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         return self.generate(temperature, beam_width, start_string,\n\u001b[0m\u001b[1;32m    435\u001b[0m                              \u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_scorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                              stoppers)[0]\n",
      "\u001b[0;32m~/Dropbox/projects/shmarkov-new/shoestrings/__init__.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, temperature, beam_width, start_string, max_tokens, custom_scorer, processors, stoppers)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             start = tuple(self.tokenizer.encode_one(start_string,\n\u001b[0m\u001b[1;32m    396\u001b[0m                                               add_special=False))\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/projects/shmarkov-new/shoestrings/tokenizers.py\u001b[0m in \u001b[0;36mencode_one\u001b[0;34m(self, item, add_special)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mIDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \"\"\"\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/projects/shmarkov-new/shoestrings/tokenizers.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, data, add_special)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     [self.vocab2idx[self.EOS]] for item in data]\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             return [self.convert_tokens_to_ids(self.tokenize(item)) for item in\n\u001b[0m\u001b[1;32m    134\u001b[0m                     data]\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/projects/shmarkov-new/shoestrings/tokenizers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     [self.vocab2idx[self.EOS]] for item in data]\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             return [self.convert_tokens_to_ids(self.tokenize(item)) for item in\n\u001b[0m\u001b[1;32m    134\u001b[0m                     data]\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/projects/shmarkov-new/shoestrings/tokenizers.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mIDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/projects/shmarkov-new/shoestrings/tokenizers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mIDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hey'"
     ]
    }
   ],
   "source": [
    "print(gen.generate_one(start_string=\"Hey Frankie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37e487",
   "metadata": {},
   "source": [
    "### A simple way to modify scores\n",
    "\n",
    "If you provide a `custom_scorer` parameter to the `.generate()` or `.generate_one()` function, the value of that parameter will be *invoked as a function* for each context and continuation under consideration during the generation process. The function call includes three parameters: the string of the continuation, the string of the entire generated sequence so far, and the probability already assigned to the token. If you're familar with this kind of thing, you can think of `custom_scorer` as a *callback* that allows you to modify the probability score of any continuation, using your own rule.\n",
    "\n",
    "If you're not familiar with this kind of thing, then here's an example to make it a bit more concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3ab87297",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: Mine\n",
      "token: has\n",
      "score: 0.0\n",
      "\n",
      "sequence: Mine has\n",
      "token: been\n",
      "score: 0.0\n",
      "\n",
      "sequence: Mine has been\n",
      "token: .\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: a\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: acted\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: consumed\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: discovered\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: done\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: dreadfully\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: hitherto\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: lying\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: my\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: passed\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: so\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been\n",
      "token: the\n",
      "score: -2.5649493574615367\n",
      "\n",
      "sequence: Mine has been lying\n",
      "token: here\n",
      "score: 0.0\n",
      "\n",
      "sequence: Mine has been lying here\n",
      "token: some\n",
      "score: 0.0\n",
      "\n",
      "sequence: Mine has been lying here some\n",
      "token: days\n",
      "score: 0.0\n",
      "\n",
      "sequence: Mine has been lying here some days\n",
      "token: I\n",
      "score: -1.3862943611198906\n",
      "\n",
      "sequence: Mine has been lying here some days\n",
      "token: for\n",
      "score: -1.3862943611198906\n",
      "\n",
      "sequence: Mine has been lying here some days\n",
      "token: spent\n",
      "score: -1.3862943611198906\n",
      "\n",
      "sequence: Mine has been lying here some days\n",
      "token: to\n",
      "score: -1.3862943611198906\n",
      "\n",
      "sequence: Mine has been lying here some days I\n",
      "token: haunted\n",
      "score: -0.6931471805599453\n",
      "\n",
      "sequence: Mine has been lying here some days I\n",
      "token: have\n",
      "score: -0.6931471805599453\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mine has been lying here some days I have'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show(token, sequence, score):\n",
    "    print(\"sequence:\", sequence)\n",
    "    print(\"token:\", token)\n",
    "    print(\"score:\", score)\n",
    "    print()\n",
    "    return score\n",
    "gen.generate_one(custom_scorer=show, max_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309a991",
   "metadata": {},
   "source": [
    "In this case, the `show()` function is being called every time the model attempts to evaluate the probability of a new token before adding it to the sequence. In my implementation of the `show()` function here, I'm printing out the parameters that are supplied to the function, so you can see how the process works. The `sequence` is the string of the sequence so far; the `token` is the string of a potential token to add next; the `score` is the probability score that the model has already assigned to that token.\n",
    "\n",
    "#### Important math aside on log probabilities\n",
    "\n",
    "The Shoestrings library uses *log probabilities* internally, as do many (if not most!) software libraries that deal with probabilities. The basic issue of working with probabilities is this. Let's say that we're calculating the probability of drawing the ace of hearts from a poker deck. The probability can be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "86ea3024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019230769230769232"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e699a",
   "metadata": {},
   "source": [
    "... or about 2%. Now let's say that we're calculating the probability of drawing the ace of hearts from a (shuffled) poker deck twice in a row. That can be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "acd9c427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00036982248520710064"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 / 52) * (1 / 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62c711",
   "metadata": {},
   "source": [
    "or about... what is that, three hundredths of a percent. If we go to *ten* times in a row, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b90e51ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9177770887761845e-18"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow((1 / 52), 10) # one over fifty two to the tenth power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f69ff",
   "metadata": {},
   "source": [
    "... which is scientific notation for about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fe089faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.917777e-18"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.000000000000000006917777"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed06828",
   "metadata": {},
   "source": [
    "... that is, `6.91777...` preceded by seventeen zeros. Not only are these numbers difficult to read, they're also difficult for computers to process—after a certain number of decimal places, it becomes difficult for computers to perform arithmetic accurately. Fortunately, we can convert very small numbers like these to a more convenient notation using the `log()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f87d0dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.951243718581427\n",
      "-7.902487437162854\n",
      "-39.51243718581427\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.log(1 / 52))\n",
    "print(np.log((1 / 52) * (1 / 52)))\n",
    "print(np.log(pow((1 / 52), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4d7e0",
   "metadata": {},
   "source": [
    "... and then retrieve the original number with the `exp()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "c3a72330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019230769230769235"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-3.951243718581427) # or 1 / 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745679f",
   "metadata": {},
   "source": [
    "So one reason to use the *log* of the probability is so we have easier-to-read and easier-to-calculate numbers. The `log()` function also has another useful property, which is that $log(x \\times y) = log(x) + log(y)$. This is useful because calculating probabilities of independent events—such as picking the same card from a shuffled poker deck twice, and picking the next token from a language model—consists of *multiplying together the probability of each event*, as we saw above with the example of drawing the ace of hearts twice in a row (i.e. $\\frac{1}{52} \\times \\frac{1}{52}$). Because of the identity above, we can actually calculate this figure as an *addition* rather than *multiplication* by adding the results of the `log()` function together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "00653627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00036982248520710064\n",
      "0.0003698224852071008\n"
     ]
    }
   ],
   "source": [
    "print((1/52)*(1/52)) # multiplying probabilities\n",
    "print(np.exp(np.log(1/52) + np.log(1/52))) # adding log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a740b",
   "metadata": {},
   "source": [
    "(Yes, the results don't match after a bunch of digits, but that's because the computer uses slightly different algorithms for calculation in the first case versus the second case—the results of these operations are proven to be mathematically identical.)\n",
    "\n",
    "Computers are *very* fast at adding numbers, and but not so fast at multiplying them, so using addition here can really speed up calculations (especially when you're calculating the probability of thousands or millions of events in a row, as is common in token prediction models).\n",
    "\n",
    "**If none of that made any sense**, you're still okay. The short explanation is this:\n",
    "\n",
    "* when a probability is represented as a log probability, a score of $0$ is equivalent to a probability of $1$\n",
    "* as a probability gets smaller, the number of its log probability goes further negative (i.e., a log probability of $-10$ represents a probability lower than $-5$)\n",
    "* if you absolutely need to know the more traditional probability of a log probability, you can convert it using `np.exp()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc30861",
   "metadata": {},
   "source": [
    "#### Back to custom scorers\n",
    "\n",
    "So back to what we were talking about: custom scoring. The probability given for each token is represented as a *log probability*. The text generation process will use whatever value our function returns as a *replacement* for the token's previously calculated log probability. That means that we can use this function to mess stuff up and boost particular tokens' probability (by adding to it) and/or diminishing the probability of others (by subtracting from it). Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4f14f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance-- or rather his sister the first time, and an awe which almost assured me that he and his dream was to all appearance dead.\n",
      "\n",
      "St. Bernard' s account, but a type of me. No one can conceive a greater degree of certainty, I arrived at a convent at Leghorn; and another darkened and sometimes in the attempt until I felt as if all hell surrounded me told me that what I am a wretch.\"\n",
      "\n",
      "Natural philosophy is the acquirement of knowledge along the precipitous sides of vast mountains of ice and should I dwell upon the beach.\n",
      "\n",
      "Does it now only am I truly thank him.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_boost(token, sequence, score):\n",
    "    if token.startswith('a'): # you can put any expression that evaluates to True or False between `if` and `:`\n",
    "        return 0\n",
    "    else:\n",
    "        return score\n",
    "for i in range(4):\n",
    "    print(gen.generate_one(custom_scorer=my_boost))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e2a84",
   "metadata": {},
   "source": [
    "What I've done here is tell the text generation function that any token that begins with `a` should have a log probability of zero (equal to a probability of $1$). Other tokens will have whatever probability had previously been assigned. You can see in the output that, indeed, there are an unnaturally large number of tokens that begin with `a` in the output!\n",
    "\n",
    "You'll notice that not all of the sentences aggressively use `a`-initial words—this is simply happenstance based on luck of the draw. This is where beam search really shines! If we increase the width of the beam search, the model's generation function will search harder for sequences that match our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "07d398ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweet and beloved as I am alone and miserable as I advanced perplexed me, and allured by the assurance of her acquittal.\"\n",
      "\n",
      "Most of the affair.\n",
      "\n",
      "All was silent and appears uneasy when anyone except myself enters his cabin.\n",
      "\n",
      "Coleridge' s account, and arm themselves for my approach.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_boost(token, sequence, score):\n",
    "    if token.startswith('a'):\n",
    "        return 0\n",
    "    else:\n",
    "        return score\n",
    "for i in range(4):\n",
    "    print(gen.generate_one(custom_scorer=my_boost, beam_width=30))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd77c13",
   "metadata": {},
   "source": [
    "You can program any criteria you want in the `my_boost()` function. Here's another example, which tries to generate sequences with words no longer than five letters, by setting any word meeting those criteria to a probability of one, and reducing the log probability of others: (Upping the temperature a bit also helps in my experience.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "872b6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uttering a few.\n",
      "\n",
      "Think not, I found it was in the house which I, and alone.\n",
      "\n",
      "Agatha, the sea.\n",
      "\n",
      "We were, and did not for me. For a long pause of the deed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_boost(token, sequence, score):\n",
    "    if len(token) <= 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return score - 5\n",
    "for i in range(4):\n",
    "    print(gen.generate_one(custom_scorer=my_boost, beam_width=30, temperature=1.5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4028e",
   "metadata": {},
   "source": [
    "Or how about generating sequences that don't include the word `the`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "106b29c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is related in them which would fully reward his toil and hazard.\n",
      "\n",
      "Geneva, May 18th, 17--\n",
      "\n",
      "Uttering a few lines in haste to say,\" Immediately upon your being taken ill, very dreadful.\"\n",
      "\n",
      "Having paid his debts, therefore, to draw inexhaustible stores of affection and mutual misfortune.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_boost(token, sequence, score):\n",
    "    if token == 'the':\n",
    "        return score - 100\n",
    "    else:\n",
    "        return score\n",
    "for i in range(4):\n",
    "    print(gen.generate_one(custom_scorer=my_boost, beam_width=30, temperature=1.5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad18e0",
   "metadata": {},
   "source": [
    "The sky is the limit! Note that you're *not* guaranteed to always produce text that meets your criteria; in a Markov chain, it's possible that the *only* possible way to continue the chain is with (for example) the word `the`, and so the model has to select it, regardless of how low its probability score is. But with a large enough corpus, and for carefully selected tasks, it should work most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f1b07",
   "metadata": {},
   "source": [
    "more stuff TK, but you can also look in the code documentation:\n",
    "\n",
    "## Using the MarkovModel class directly\n",
    "\n",
    "## Tokenizers\n",
    "\n",
    "## Processors\n",
    "\n",
    "## Stoppers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
